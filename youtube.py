# -*- coding: utf-8 -*-
"""Youtube Comments Spam Detection | f1-score : 96% âœ…

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/youtube-comments-spam-detection-f1-score-96-38589cca-4885-46a1-914b-cabfc98fdc05.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240906/auto/storage/goog4_request%26X-Goog-Date%3D20240906T200643Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dcfa4bfdc6c4168ebc17db05e6c1723086bc7cc5edd67b340058a801c5c8b06c317df39001e50c52fdedc56376ac2eadb4ec4194dbf36579e6857c8a30d28d6f017a02fe754f8718b6fe1d7a1e886d7e0037ec1dadd6cdee9ad8f2991f55340f13ca828763047a1de851033e4d103fcb322792612c97c2b443bb537e6fb0b73d20489161867de174fbf176241e9baf168411ad856ce912f5eba93b5ae7503ae5524758b39d96a25551eb4dff2b0ae2562564fcb22672be0f8f0e6ea90d12a0810f832e6173b8a0f88d4aa099cd77e0ceb0b10f30bf6c624d3d039b0d4fe5282f9bafc5cdfe9bd627d7adda0ec5f76bb255ecdc0c731bd5b169f7ef0f9088e8125

# Import Necessary Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud

import warnings
warnings.filterwarnings("ignore")

"""# Load the Data"""

df = pd.read_csv("/content/Youtube-Spam-Dataset.csv")

df.head()

"""# Sanity Check of Data"""

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df['CLASS'].value_counts()

"""# Exploratory Data Anlaysis (EDA)"""

plt.figure(figsize=(10, 6))
sns.barplot(x=df['AUTHOR'].value_counts().head(10).values,
            y=df['AUTHOR'].value_counts().head(10).index, palette='viridis')
plt.title('Top 10 Most Frequent Comment Authors', fontsize=16)
plt.xlabel('Number of Comments')
plt.ylabel('Author')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x=df['VIDEO_NAME'].value_counts().head(5).values,
            y=df['VIDEO_NAME'].value_counts().head(5).index, palette='viridis')
plt.title('Top 10 Most Frequent Videos', fontsize=16)
plt.xlabel('Number of Videos')
plt.ylabel('Video')
plt.show()

"""## What days were the most comments written?"""

df["DATE"] = df["DATE"].apply(lambda x: str(x).split('T')[0])

df["DATE"].value_counts().sort_values(ascending=False)[1:5]

spam_counts = df['CLASS'].value_counts().sort_index()

palette = sns.color_palette("Blues", n_colors=len(spam_counts))
fig = plt.figure(figsize=(10, 6))
plt.pie(spam_counts, autopct='%1.1f%%', startangle=140, colors=palette, labels=["Not Spam", "Spam"])
plt.tight_layout()
plt.show()

spam_comments = ' '.join(df[df['CLASS'] == 1]['CONTENT'])
spam_wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(spam_comments)

plt.figure(figsize=(10, 6))
plt.imshow(spam_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud for Spam Comments', fontsize=16)
plt.tight_layout()  # Adjust the layout
plt.show()

non_spam_comments = ' '.join(df[df['CLASS'] == 0]['CONTENT'])
non_spam_wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='Blues').generate(non_spam_comments)

plt.figure(figsize=(10, 6))
plt.imshow(non_spam_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud for Non-Spam Comments', fontsize=16)
plt.show()

df['Comment Length'] = df['CONTENT'].apply(len)

plt.figure(figsize=(10, 6))
sns.histplot(df, x='Comment Length', hue='CLASS', multiple='stack', palette='Reds')
plt.title('Distribution of Comment Lengths by Class', fontsize=16)
plt.xlabel('Comment Length (in characters)', fontsize=14)
plt.ylabel('Frequency')
plt.show()

"""# Garbage Treatment"""

df.drop_duplicates(inplace=True)

"""# Feature Selection"""

df["comment_info"] = df["AUTHOR"] + "  " + df["VIDEO_NAME"] + "  " + df["CONTENT"]

X = df["comment_info"]
y = df["CLASS"]

"""# Encoding"""

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(X)

X.shape

"""# Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# Model Building"""

clf = LinearSVC(random_state=42)

clf.fit(X_train, y_train)

"""# Evaluation"""

y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='viridis')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')
plt.show()